/******************************************************************************
** Lab 3: Plagiarism detection
******************************************************************************/

Group members:
- [Andreas Åberg]
- [Lukas Gunterberg-Klase]
- [Alexander Hedén]

/******************************************************************************
** Task 1: Complexity analysis
**
** 1. What is the asymptotic complexity of find_similarity?
**    Answer in terms of N, the total number of 5-grams in the input files.
**    Assume that the number of duplicate occurrences of 5-grams is
**    a small constant - that is, there is not much plagiarised text.
**    Explain briefly.
******************************************************************************/

"Number of handshakes" problem making the comparisons between files O((D^2-D)/2) = O(D^2)
same for 5-grams = O(K^2).
O(D^2*K^2) = O(N^2)

Measured:
tiny = 5.01s
small = 49.54s

Calculated:
small_ngram/tiny_ngram = 3.333333, about 10 times the time (3.33^2).

/******************************************************************************
** 2. How long did the program take on the 'tiny' and 'small' directories?
**    Is the ratio between the times what you would expect, given the complexity?
**    Explain very briefly why.
******************************************************************************/

Measured:
tiny = 5.01s
small = 49.54s

Calculated:
small_ngram/tiny_ngram = 3.333333, about 10 times the time (3.33^2).

Seems about right.

/******************************************************************************
** 3. How long do you predict the program would take to run on
**    the 'big' directory? Show your calculations.
******************************************************************************/

If small takes about 50 seconds and the factor big/small = 100, factor= 100^2 = 10000.
It will take about 10000 times the time of small, which is roughly
500 000 seconds = 139 hours.

/******************************************************************************
** Task 2: Make use of an index
**
** 4. Now what is the total asymptotic complexities of running and build_index
**    and find_similarity? Include brief justification. Again, assume a total
**    of N 5-grams, and a constant number of duplicate occurrences of 5-grams.
******************************************************************************/

build_index will add all 5-grams to a table. This will take O(N) time since its
just adding without any ordering.

find_similarity will check all n-grams in the index dictionary O(N).
It will then count all pairs of paths for every N-gram with more than one occurrence.
If we have a constant number of files (k << number of Ngrams) and we have a collision
with every file this will result in O(k*N) = O(N) time at most.

/******************************************************************************
** 5 (optional). What if the total similarity score is an arbitrary number S,
**               rather than a small constant?
******************************************************************************/

As the similarity increase S->N, the complexity will close in on O(N^2)?


/******************************************************************************
** Task 3: Implement hash tables with linear probing
**
** 6. Run lab3.py on the big document set and, with the help of the statistics
**    it prints out, answer this question:
**
**    Assume that we call `index.get` on a random key which is present in
**    the hash table. How many array accesses are needed on average to
**    find the key? And how many in the worst case? Explain how you got
**    your answer.
**
**    Include the hash table statistics from running `lab3.py documents/big`
**    in your answer. Answer with a number calculated for that document set.
******************************************************************************/

Hash table statistics:
  files: Hash table, size 953, capacity 3072, load factor 0.31, avg distance 0.20, max distance 4
  index:  Hash table, size 2061519, capacity 6291456, load factor 0.33, avg distance 0.24, max distance 18
  similarity: Hash table, size 53168, capacity 196608, load factor 0.28, avg distance 0.19, max distance 12

  If the average distance is 0 we will need 1 array access to find a key.
  The average distance is 0.24, which means mostly 1 accesses. (Or 1.24 accesses we guess?).
  In the worst case we have 18 max distance, which means 19 array accesses.


/******************************************************************************
** Task 4: Improve a hash function
**
** 7. How did you improve the hash function?
**    Briefly explain why your design gives a better distribution of hash
**    codes than the bad hash function.
******************************************************************************/

Since order does not matter for addition we have to make sure that permutations of
words for n-grams with the same words do not collide when hashed.
The number 31 seems to work good as a multiplier when iterating over the string
according to Data Structures and Algorithms in Java by Michael T. Goodrich
when they checked 50 000 english words.

We want a prime number as multiplier because of mod 2^32
To improve the hashing you could also bitshift the hash with 5 (32*hash) and then
remove 1 hash to get 31 (prime). This is probably bad practice in Python since the compiler
probably improves multiplication anyway (so its probably insignificantly slightly better/worse).

The starting value of hash = 5381 is also prime, its not as important
just a tribute to Dan Bernstein.

/******************************************************************************
** Appendix: General information
**
** A. Approximately how many hours did you spend on the assignment?
******************************************************************************/

Andreas Åberg: 10 hours
Alexander Hedén: 10 hours
Lukas Gunterberg-Klase: 10 hours

/******************************************************************************
** B. Are there any known bugs / limitations?
******************************************************************************/

We are limited of hashing values of words. You would probably use different hashes
for a different set of datatypes.
(For integers the identity function would be great hash since it has a uniform
distribution with no collisions. )

/******************************************************************************
** C. Did you collaborate with any other students on this lab?
**    If so, please write in what way you collaborated and with whom.
**    Also include any resources (including the web) that you may
**    may have used in creating your design.
******************************************************************************/

The hashing function of Python in C-code
Data Structures and Algorithms in Java by Michael T. Goodrich
Lecture videos on Google-Drive

/******************************************************************************
** D. Describe any serious problems you encountered.                    
******************************************************************************/


/******************************************************************************
** E. List any other comments here.
**    Feel free to provide any feedback on how much you learned 
**    from doing the assignment, and whether you enjoyed it.                                             
******************************************************************************/
One of the most enjoyable labs because we are way more familiar with the course by now.
Great fun.
